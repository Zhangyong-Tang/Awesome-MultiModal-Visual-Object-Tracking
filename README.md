#  A Comprehensive Survey for Multi-Modal Visual Object tracking

<p align="center">
<img src="https://github.com/Zhangyong-Tang/MultiModal-Visual-Object-tracking/blob/main/mind-1014-white.png" width="800">
</p>

#### We appreciate any efforts committed to the development of Multi-Modal Visual Object Tracking. Please feel free to connect us (zhangyong_tang_jnu@163.com,tianyang.xu@jiangnan.edu.cn) for discussion or missed works.


## üí•: Update Log 
* [2024.07.02] ALL paper and code links added. Our paper is coming soon.
* [2024.06.20] Paper and code links for RGBT papers.
* [2024.06.14] The classification of all the trackers are added.
* [2024.06.01] All the multi-modal tracking papers are added.
* [2024.05.29] All datasets are listed.
* [2024.05.28] All survey papers and RGBT papers are listed.
* [2023.12.16] The repository is started.

## üèÉ: Find Results Here
* [RGBT-Tracking-Results-Datasets-and-Methods](https://github.com/Zhangyong-Tang/RGBT-Tracking-Results-Datasets-and-Methods)
* [RGBD-Tracking-Results-Datasets-and-Methods](https://github.com/Zhangyong-Tang/RGBD-Tracking-Results-Datasets-and-Methods)
* [RGBE-Tracking-Results-Datasets-and-Methods](https://github.com/Zhangyong-Tang/RGBE-Tracking-Results-Datasets-and-Methods)
* [RGBL-Tracking-Results-Datasets-and-Methods](https://github.com/Zhangyong-Tang/RGBL-Tracking-Results-Datasets-and-Methodss)
* [RGBNIR/S-Tracking-Results-Datasets-and-Methods](https://github.com/Zhangyong-Tang/MultiModal-Visual-Object-tracking)

## :point_right: : Our contributions to the MMVOT community
* UniMod1K: Towards a More Universal Large-Scale Dataset and Benchmark for Multi-modal Learning. Xue-Feng Zhu, Tianyang Xu, Zongtao Liu, Zhangyong Tang, Xiao-Jun Wu & Josef Kittler. IJCV 2024. [[Paper](https://link.springer.com/article/10.1007/s11263-024-01999-8)] [[Code](https://github.com/xuefeng-zhu5/UniMod1K)]
* Generative-based Fusion Mechanism for Multi-Modal Tracking. Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu*, Josef Kittler. AAAI 2024.  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28325)] [[Code](https://github.com/Zhangyong-Tang/GMMT-AAAI2024)].
* Multi-Level Fusion for Robust RGBT Tracking via Enhanced Thermal Representation. Zhangyong Tang, Tianyang Xu, Xiaojun Wu, Josef Kittler. ACMTOMM 2024. [[Paper](https://dl.acm.org/doi/abs/10.1145/3678176)] [[Code](https://github.com/Zhangyong-Tang/MELT)]. MELT
* Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking. Shao, Pengcheng and Xu, Tianyang and Zhu, Xuefeng and Wu, Xiaojun and Kittler, Josef. PRCV 2024.  Best Paper Nomination Award. [[Paper](https://arxiv.org/abs/2409.17560)] [Code]. DS-MESA
* TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking. Pengcheng Shao, Tianyang Xu, Zhangyong Tang, Linze Li, Xiao-Jun Wu, Josef Kittler. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.05004)] [[Code](https://github.com/SSSpc333/TENet)].
* Feature enhancement and coarse-to-fine detection for RGB-D tracking. Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu, Josef Kittler. PRL 2024. FECD.
* Adaptive Colour-Depth Aware Attention for RGB-D Object Tracking. Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu, Zhenhua Feng, Josef Kittler. SPL 2024. CDAAT. [[Paper](https://ieeexplore.ieee.org/abstract/document/10472092)] [[Code](https://github.com/xuefeng-zhu5/CDAAT)].
* Self-supervised learning for RGB-D object tracking. Xue-Feng Zhu, Tianyang Xu, Sara Atito, Muhammad Awais, Xiao-Jun Wu, Zhenhua Feng, Josef Kittler. PR 2024. SSLTrack.
* RGBD1K: A Large-Scale Dataset and Benchmark for RGB-D Object Tracking. Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, ZuchengWu, Haodong Liu, Xiao Yang, Xiao-Jun Wu, Josef Kittler. AAAI 2023. RGBD1K. [[Paper](https://arxiv.org/abs/2208.09787)] [[Code](https://github.com/xuefeng-zhu5/RGBD1K)].
* Exploring fusion strategies for accurate RGBT visual object tracking. Zhangyong Tang, Tianyang Xu, Hui Li, Xiao-Jun Wu, XueFeng Zhu, Josef Kittler. Information Fusion 2023. DFAT. [[Paper](https://www.sciencedirect.com/science/article/pii/S1566253523001975)]. [[Code](https://github.com/Zhangyong-Tang/DFAT-Information-Fusion2023)].
* Temporal Aggregation for Adaptive RGBT Tracking. Tang, Zhangyong and Xu, Tianyang and Wu, Xiao-Jun. Arxiv 2022. [[Paper](https://arxiv.org/abs/2201.08949)] [[Code](https://github.com/Zhangyong-Tang/TAAT)]. TAAT
* <font color=red>ËøôÊÆµÊñáÂ≠óÊòØÁ∫¢Ëâ≤ÁöÑ</font>


## :punch: : Find what you want quickly

[RGBT Tracking Papers](#RGB-T-Tracking)

[RGBD Tracking Papers](#RGB-D-Tracking)

[RGBE Tracking Papers](#RGB-E-Tracking)

[RGBL Tracking Papers](#RGB-L-Tracking)

[RGBNIR Tracking Papers](#RGB-NIR-Tracking)

[RGBS Tracking Papers](#RGB-S-Tracking)

[RGB+Multi Tracking Papers](#RGB-Multi-Tracking)

## :punch: :Datasets and Benchmarks

### RGB-Mutli Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| UniMod1K| IJCV'2024 |[UniMod1K](https://github.com/xuefeng-zhu5/UniMod1K) |RGB+D+L: UniMod1K: Towards a More Universal Large-Scale Dataset and Benchmark for Multi-modal Learning|
| WebUAV-3M| TPAMI'2023 | [WebUAV-3M](https://github.com/983632847/WebUAV-3M) | RGB+L+Audio: WebUAV-3M: A Benchmark for Unveiling the Power of Million-Scale Deep UAV Tracking|

### RGBT Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| MV-RGBT| Arxiv'2024 |[MV-RGBT]() |Revisiting RGBT Tracking Benchmarks from the Perspective of Modality Validity: A New Benchmark, Problem, and Method|
| VTUAV| CVPR'2022 |[VTUAV](https://zhang-pengyu.github.io/DUT-VTUAV/) |Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline|
| LasHeR| TIP'2021 | [LasHeR](https://github.com/BUGPLEASEOUT/LasHeR) |LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking|
| VOT-RGBT20| VOT CommunityÔºö 2020 | [VOT-RGBT2020](https://pan.baidu.com/s/1fNgAVk4siqP2p-b1M2ZGmg) *CODE:TZYD*| The Eighth Visual Object Tracking VOT2020 Challenge Results|
| VOT-RGBT19| VOT CommunityÔºö2019 |[VOT-RGBT2019](https://pan.baidu.com/s/1kYnTTWF9LIkrCH4NNsSlFQ) *CODE:TZYD* | The Seventh Visual Object Tracking VOT2019 Challenge Results|
| RGBT234| PR'2018 | [RGBT234](https://sites.google.com/view/ahutracking001/)|RGB-T object tracking: Benchmark and baseline|
| RGBT210| ACM MM'2017 | [RGBT210](https://drive.google.com/file/d/0B3i2rdXLNbdUTkhsLVRwcTBTMlU/view?resourcekey=0-vytg_w3hqlQfLhoiS2J8Dg) |Weighted Sparse Representation Regularized Graph Learning for RGB-T Object Tracking|
| GTOT | TIP'2016 | [GTOT](https://pan.baidu.com/s/1QNidEo-HepRaS6OIZr7-Cw) |Learning Collaborative Sparse Representation for Grayscale-Thermal Tracking|
| LITIV | CVIU'2012 | [LITIV](https://www.polymtl.ca/litiv/en/codes-and-datasets) |An iterative integrated framework for thermal‚Äìvisible image registration, sensor fusion, and people tracking for video surveillance applications|
| OTCBVS | CVIU'2007 | [OTCBVS](http://vcipl-okstate.org/pbvs/bench/) |Background-subtraction using contour-based fusionof thermal and visible imagery|
| LSS-Dataset(from RGB) | TCSVT'2021 | [LSS-Dataset](https://pan.baidu.com/s/1x2hiqb2lSo54_4CI_L9YeQhttps://pan.baidu.com/s/1x2hiqb2lSo54_4CI_L9YeQ) ,code(Ye5Q)|SiamCDA: Complementarity-and distractor-aware RGB-T tracking based on Siamese network|
| LSS-Dataset(from TIR) | TCSVT'2021 | [LSS-Dataset](https://pan.baidu.com/s/1xD3Ox-9VbZnyRQSWOxQRNw),code(IHws) |SiamCDA: Complementarity-and distractor-aware RGB-T tracking based on Siamese network|


### RGBD Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| D2CUBE| CVRP'2023|[D2CUBE](https://github.com/yjybuaa/RGBDAerialTracking)|Resource-Efficient RGBD Aerial Tracking|
| ARKittrack| CVPR'2023 | [ARKittrack](https://github.com/lawrence-cj/ARKitTrack) |ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data|
| RGBD1K | AAAI'2023 | [RGBD1K](https://github.com/xuefeng-zhu5/RGBD1K) |RGBD1K: A Large-Scale Dataset and Benchmark for RGB-D Object Tracking|
| VOT-RGBD2022 | VOT Community | [VOT-RGBD2022](https://www.votchallenge.net/vot2022/dataset.html) |The Tenth Visual Object Tracking VOT2022 Challenge Results|
| DepthTrack | ICCV'2021 | [DepthTrack](https://github.com/xiaozai/DeT) |DepthTrack: Unveiling the Power of RGBD Tracking|
| CDTB | ICCV'2019 | [CDTB](https://www.votchallenge.net/vot2019/dataset.html) |CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark|
| STC | ICCV'2019 | [STC](https://pan.baidu.com/s/1Y3z2JH-oR68-stWFVnHUVw) code:TZYD|Robust Fusion of Color and Depth Data for RGB-D Target Tracking Using Adaptive Range-Invariant Depth Models and Spatio-Temporal Consistency Constraints|
| PTB | ICCV'2013 | [PTB](https://tracking.cs.princeton.edu/index.html) |Tracking Revisited using RGBD Camera: Unified Benchmark and Baselines|
| BoBoT | - | [BoBoT](http://www.iai.uni-bonn.de/~kleind/tracking/index.htm ) |BoBot - Bonn benchmark on tracking|

### RGBE Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| CRSOT| Arxiv'2024 |[CRSOT](https://github.com/Event-AHU/Cross_Resolution_SOT) |CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras|
| FELT| Arxiv'2024 |[FELT](https://github.com/Event-AHU/FELT_SOT_Benchmark) |Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline|
| COESOT| Arxiv'2022 | [COESOT](https://github.com/Event-AHU/COESOT)|Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric|
| VisEvent| TCYB'2023 |[VisEvent](https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark?tab=readme-ov-file)|VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows|
| FE108/FE240hz| ICCV'2021 | [FE108/FE240hz](https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking)|Object Tracking by Jointly Exploiting Frame and Event Domain|
| EED| Arxiv'2018 | [EED](http://prg.cs.umd.edu/BetterFlow.html)|Event-based Moving Object Detection and Tracking|



### RGBL Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| VLT-MI| Arxiv'2024 |[VLT-MI]() |Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark|
| ElysiumTrack-1M| Arxiv'2024 |[ElysiumTrack-1M](https://github.com/Hon-Wong/Elysium) |Elysium: Exploring Object-level Perception in Videos via MLLM|
| WebUOT-1M| Arxiv'2024 |[WebUOT-1M](https://github.com/983632847/Awesome-Multimodal-Object-Tracking) |WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark|
| VastTrack| Arxiv'2024 |[VastTrack](https://github.com/HengLan/VastTrack) |VastTrack: Vast Category Visual Object Tracking|
| MGIT| NIPS'2023 |[MGIT](http://videocube.aitestunion.com/) |A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship|
| TNL2K| CVPR'2021 | [TNL2K](https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit)|Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark|
| LaSOT_EXT| IJCV'2021 |[LaSOT_EXT](https://github.com/HengLan/LaSOT_Evaluation_Toolkit)|LaSOT: A High-quality Large-scale Single Object Tracking Benchmark|
| LaSOT| CVPR'2019 | [LaSOT](https://github.com/HengLan/LaSOT_Evaluation_Toolkit)|LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking|
| OTB99-L| CVPR'2017 | [OTB99-L](https://github.com/QUVA-Lab/lang-tracker) |Tracking by Natural Language Specification|


### RGBNIR Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| CMOTB| TNNLS'2024 |[CMOTB](https://github.com/mmic-lcl/Datasets-and-benchmark-code) |Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset|


### RGBS Datasets

| Dataset | Publish  | GitHub| Introduction|
|--|--|--| --|
| RGBS50| Arxiv'2024 |[RGBS50](https://github.com/LiYunfengLYF/RGBS50) |RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer Tracker|


## :star2: :Surveys and Report
* RGBT ---- A Comprehensive Review of RGBT Tracking. Haiping Zhang, Di Yuan, Xiu Shu, Zhihui Li, Qiao Liu, Xiaojun Chang, Zhenyu He, and Guangming Shi. TIM 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10616144)]
* RGBT ---- RGBT tracking: A comprehensive review. Mingzheng Feng, Jianbo Su*. Information Fusion 2024. [[Paper](https://www.sciencedirect.com/science/article/pii/S1566253524002707)]
* RGBT ---- Review and Analysis of RGBT Single Object Tracking Methods: A Fusion Perspective. Zhihao Zhang, Jun Wang, Zhuli Zang, Lei Jin, Shengjie Li, Hao Wu*,Jian Zhao, Zhang Bo. ACM TOMM 2024. [[Paper](https://dl.acm.org/doi/10.1145/3651308)]
* RGBT---- Object fusion tracking based on visible and infrared images: A comprehensive review. Xingchen Zhang, Ping Ye, Henry Leung, Ke Gong, Gang Xiao*. Information Fusion 2020. [[Paper](https://www.sciencedirect.com/science/article/pii/S1566253520302657)]
* RGBT---- A Survey for Deep RGBT Tracking. Zhangyong Tang, Tianyang Xu, and Xiao-Jun Wu*. Arxiv 2022. [[Paper](https://arxiv.org/abs/2201.09296)]
* RGBD---- A Survey of RGB-Depth Object Tracking. Zhou Ou, Ge Ying, Dawei Zhang*, Zhonglong Zheng. Journal of Computer-Aided Design & Computer Graphics 2024. [[Paper](https://www.jcad.cn/en/article/doi/10.3724/SP.J.1089.null.2023-00537)]
* RGBD---- Rgbd object tracking: An in-depth review. Jinyu Yang, Zhe Li, Song Yan, Feng Zheng*, Ale≈° Leonardis, Joni-Kristian K√§m√§r√§inen, Ling Shao. Arxiv 2022. [[Paper](https://arxiv.org/abs/2203.14134)]
* RGBD/T ---- Multi-modal visual¬†tracking: Review and experimental comparison. Zhang, Pengyu, Dong Wang*, and Huchuan Lu. Computational Visual Media 2024. [[Paper](https://link.springer.com/article/10.1007/s41095-023-0345-5)]
* RGBD/T/E/L ---- Awesome Multi-modal Object Tracking (MMOT). Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang. Arxiv 2024. [[Paper](https://github.com/983632847/Awesome-Multimodal-Object-Tracking?tab=readme-ov-file#datasets)]




## :star: :Regular Papers 

### Unified (Model or Architecture for) Multi-Modal Tracking
* AMATrack: A Unified Network With Asymmetric Multimodal Mixed Attention for RGBD Tracking. Ping Ye , Gang Xiao , and Jun Li. TIM 2024. [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10623547)] [[Code]()] AMATrack
* MixRGBX: Universal multi-modal tracking with symmetric mixed attention. Meng Sun, Xiaotao Liu, Hongyu Wang, Jing Liu. Neurocomputing 2024. [[Paper](https://www.sciencedirect.com/science/article/pii/S0925231224010452)] [[Code]()].
* Towards a Generalist and Blind RGB-X Tracker. Yuedong Tan, Zongwei Wu, Yuqian Fu, Zhuyun Zhou, Guolei Sun, Chao Ma, Danda Pani Paudel, Luc Van Gool, Radu Timofte. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.17773)] [[Code](https://github.com/supertyd/XTrack)]. XTrack.
* Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline. Xiao Wang, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong Tian, Jin Tang, and Bin Luo. Arxiv 2024. [[Paper](https://arxiv.org/abs/2403.05839)] [[Code](https://github.com/Event-AHU/FELT_SOT_Benchmark)] AMTTrack
* Unified Sequence-to-Sequence Learning for Single- and Multi-Modal Visual Object Tracking. Xin Chen, Ben Kang, Jiawen Zhu, Dong Wang*, Houwen Peng, and Huchuan Lu. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2304.14394)] [[Code](https://github.com/chenxin-dlut/SeqTrackv2)] SeqTrackv2
* SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking. Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen. CVPR 2024. [[Paper](https://arxiv.org/pdf/2403.16002)] [[Code](https://github.com/hoqolo/SDSTrack)] SDSTrack
* Single-Model and Any-Modality for Video Object Tracking. Zongwei Wu, Jilai Zheng, Xiangxuan Ren, Florin-Alexandru Vasluianu, Chao Ma*, Danda Pani Paudel, Luc Van Gool, Radu Timofte. CVPR 2024. [[Paper](https://arxiv.org/abs/2311.15851)] [[Code](https://github.com/Zongwei97/UnTrack)]
* OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning. Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang*. CVPR 2024. [[Paper](https://arxiv.org/pdf/2403.09634)] [Code]
* Knowledge Synergy Learning for Multi-Modal Tracking. He, Yuhang and Ma, Zhiheng and Wei, Xing and Gong, Yihong. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10388341)] [Code]. KSTrack.
* MINet: Modality interaction network for unified multi-modal tracking. Shuang Gong, Zhu Teng1*, Rui Li, Jack Fan, Baopeng Zhang, Jianping Fan. IVC 2024. [[Paper (https://www.sciencedirect.com/science/article/pii/S0262885624001756)] [Code]
* Visual Prompt Multi-Modal Tracking. Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang*, Huchuan Lu. CVPR 2023.  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf)] [[Code](https://github.com/jiawen-zhu/ViPT)]
* Feature Disentanglement and Adaptive Fusion for Improving Multi-modal Tracking. Zheng Li, Weibo Cai, Junhao Dong, Jianhuang Lai, and Xiaohua Xie. PRCV 2023 [[Paper](https://link.springer.com/chapter/10.1007/978-981-99-8555-5_6)] [[Code](https://github.com/ccccwb/
Multimodal-Detection-and-Tracking-UAV)]. FDAFT
* Prompting for multi-modal tracking. Yang, Jinyu and Li, Zhe and Zheng, Feng and Leonardis, Ales and Song, Jingkuan. ACM MM 2022. [[Paper](https://dl.acm.org/doi/abs/10.1145/3503161.3547851)] [Code]. ProTrack

### RGB-T Tracking
2024
* AFter: Attention-based Fusion Router for RGBT Tracking. Andong Lu, Wanyu Wang, Chenglong Li, Jin Tang, Bin Luo. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.02717)] [[Code](https://github.com/Alexadlu/AFter)]. AFter
* A content-aware correlation filter with multi-feature fusion for RGB-T tracking. FENG Zihang, YAN Liping*, BAI Jinglan, XIA Yuanqing, and XIAO Bo. Journal of Systems Engineering and Electronics 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10530492)] [Code]. CAFF
* AMNet: Learning to Align Multi-modality for RGB-T Tracking. Zhang Tianlu, He Xiaoyi, Jiao Qiang, Zhang Qiang, Han Jungong. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10472533)] [Code]. AMNet.
* Breaking Modality Gap in RGBT Tracking: Coupled Knowledge Distillation. Andong Lu, Jiacong Zhao, Chenglong Li, Yun Xiao, Bin Luo. ACMMM 2024. [[Paper](https://openreview.net/forum?id=2jzyYyRqX0)] [[Code](https://github.com/Alexadlu/CKD)]. CKD.
* Bi-directional Adapter for Multi-modal Tracking. Bing Cao, Junliang Guo, Pengfei Zhu, Qinghua Hu. AAAI 2024. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/27852)] [[Code](https://github.com/SparkTempest/BAT)]. BAT
* Cross-modulated Attention Transformer for RGBT Tracking. Yun Xiao, Jiacong Zhao, Andong Lu, Chenglong Li, Yin Lin, Bing Yin, Cong Liu. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2408.02222)] [[Code]()]. CAFormer
* Cross Fusion RGB-T Tracking with Bi-directional Adapter. Zhirong Zeng, Xiaotao Liu*, Meng Sun, Hongyu Wang, Jing Liu. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2408.16979)] [[Code]()]. CFBT
* Exploring target-related information with reliable global pixel relationships for robust RGB-T tracking. Tianlu Zhang, Xiaoyi He, Yongjiang Luo, Qiang Zhang, Jungong Han. PR 2024. [[Paper](https://www.sciencedirect.com/science/article/pii/S0031320324004588)] [[Code](https://github.com/Tianlu-Zhang/SiamTIH)]. TIH
* Exploring Multi-modal Spatial-Temporal Contexts for High-performance RGB-T Tracking. Tianlu Zhang , Qiang Jiao , Qiang Zhang and Jungong Han. TIP 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10605602)] [Code]. MMSTC.
* From Two Stream to One Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation. Luo, Yang, Xiqing Guo, and Hao Li. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2403.16834)] [Code]. MPLKD
* Generative-based Fusion Mechanism for Multi-Modal Tracking. Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu*, Josef Kittler. AAAI 2024.  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28325)] [[Code](https://github.com/Zhangyong-Tang/GMMT-AAAI2024)]. GMMT
* Learning Multi-Frequency Integration Network for RGBT Tracking. Jiatian Mei, Juxiang Zhou, Jun Wang, Jia Hao, Dongming Zhou, and Jinde Cao. IEEE Sensor Journal 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10458005)] [[Code](https://github.com/mjt1312/Lminet)]. LMINet
* Modality-missing RGBT Tracking: Invertible Prompt Learning and High-quality Benchmarks. Andong Lu, Jiacong Zhao, Chenglong Li, Jin Tang, Bin Luo. Arxiv 2024. [[Paper](https://arxiv.org/abs/2312.16244)] [[Code](https://github.com/Alexadlu/Modality-missing-RGBT-Tracking)]. IPL
* MambaVT: Spatio-Temporal Contextual Modeling for robust RGB-T Tracking. Simiao Lai1, Chang Liu1, Jiawen Zhu1, Ben Kang1, Yang Liu1, Dong Wang1, Huchuan Lu. Arxiv 2024. [[Paper](https://arxiv.org/abs/2408.07889)] [[Code]
* Maximize Peak-to-Sidelobe Ratio for Real-Time RGB-T Tracking. Xu Zhu, Jun Liu, Xingzhong Xiong, and Zhongqiang Luo. TIM 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10385185)] [[Code](https://github.com/Liujunzx/MPT)]. MPT
* Multi-modal Interaction with Token Division Strategy for RGB-T Tracking. Yujue Caia, Xiubao Suia, Guohua Gua, Qian Chen. PR 2024.[[Paper](https://www.sciencedirect.com/science/article/pii/S0031320324003777)] [Code]. MIGTD
* Multi-Level Fusion for Robust RGBT Tracking via Enhanced Thermal Representation. Zhangyong Tang, Tianyang Xu, Xiaojun Wu, Josef Kittler. ACMTOMM 2024. [[Paper](https://dl.acm.org/doi/abs/10.1145/3678176)] [[Code](https://github.com/Zhangyong-Tang/MELT)]. MELT
* MATI: Multimodal Adaptive Tracking Integrator for Robust Visual Object Tracking. Kai Li, Lihua Cai, Guangjian He and Xun Gong. Sensors 2024. [[Paper](https://www.mdpi.com/1424-8220/24/15/4911)] [Code]. MATI
* Multi-scale feature extraction and fusion with attention interaction for RGB-T tracking. Haijiao Xing, Wei Wei, Lei Zhang, Yanning Zhang. PR 2024. [[Paper](https://www.sciencedirect.com/science/article/pii/S003132032400668X)] [Code]. MFATrack
* Motion-aware Self-supervised RGBT Tracking with Multi-modality Hierarchical Transformers. SHENGLAN LI, RUI YAO‚àó, YONG ZHOU, HANCHENG ZHU, JIAQI ZHAO, and ZHIWEN SHAO, ABDULMOTALEB EL SADDIK. ACMTOMM 2024. [[Paper](https://dl.acm.org/doi/10.1145/3698399)] [Code]. S2OTFormer
* QueryTrack: Joint-Modality Query Fusion Network for RGBT Tracking. Fan, Huijie and Yu, Zhencheng and Wang, Qiang and Fan, Baojie and Tang, Yandong. TIP 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10516307)] [Code] QueryTrack
* Revisiting RGBT Tracking Benchmarks from the Perspective of Modality Validity: A New Benchmark, Problem, and Method. Zhangyong Tang, Tianyang Xu, Zhenhua Feng, Xuefeng Zhu, He Wang, Pengcheng Shao, Chunyang Cheng, Xiao-Jun Wu‚àó, Muhammad Awais, Sara Atito, and Josef Kittler. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.00168)] [[Code](https://github.com/Zhangyong-Tang/MVRGBT)] MV-RGBT
* RGBT Tracking via Challenge-Based Appearance Disentanglement and Interaction.  Liu, Lei and Li, Chenglong and Xiao, Yun and Ruan, Rui and Fan, Minghao. TIP 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10460420)] [[Code](https://github.com/liulei970507/CAT-TIP)] CAT++.
* RGB-T tracking of efficient feature maps via dual-stream Siamese network. Jinlong Li, Rui Li. ICCGIV 2024.  [[Paper](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13288/1328816/RGB-T-tracking-of-efficient-feature-maps-via-dual-stream/10.1117/12.3045422.short#_=_)] [Code] SiamEFM
* Real-Time RGBT Target Tracking Based on Attention Mechanism. Qian Zhao, Jun Liu, Junjia Wang and Xingzhong Xiong. electronics 2024. [[Paper](https://www.mdpi.com/2079-9292/13/13/2517)] [Code] AMRT
* RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning. Yabin Zhu, Chenglong Li, Xiao Wang, Jin Tang, Zhixiang Huang. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10506555)].  [Code] Proformer
* RGB-T Tracking with Template-Bridged Search Interaction and Target-Preserved Template Updating. Bo Li, Fengguang Peng, Tianrui Hui, Xiaoming Wei, Xiaolin Wei, Lijun Zhang, Hang Shi, Si Liu. TPAMI 2024.  [[Paper](https://ieeexplore.ieee.org/abstract/document/10706882?casa_token=eT7nrpk8Hj0AAAAA:k42jTwWcqMC0mQ0GvP0Ly5e5ukxJW1iCVrn932UYl-1iz1cjaJ3AbjctpujJcllLdjXcQ4w6nUA)] [[Code](https://github.com/RyanHTR/TBSI)]. TBSI-E
* Robust RGB-T Tracking via Adaptive Modality Weight Correlation Filters and Cross-modality Learning. Mingliang Zhou, Xinwen Zhao, Futing Luo, Jun Luo, Huayan Pu, Tao Xiang. ACM TOMM 2024. [[Paper](https://dl.acm.org/doi/full/10.1145/3630100)] [[Code](https://github.com/LDating/AWCM)]. AWCM
* RGBT Tracking via All-layer Multimodal Interactions with Progressive Fusion Mamba. Andong Lu‚àó, Wanyu Wang‚Ä†, Chenglong Li‚Ä†, Jin Tang‚àó and Bin Luo. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2408.08827)].  [Code] AINet
* SiamMGT: robust RGBT tracking via graph attention and reliable modality weight learning. Lizhi Geng, Dongming Zhou, Kerui Wang, Yisong Liu, Kaixiang Yan. The Journal of Supercomputing 2024. [[Paper](https://link.springer.com/article/10.1007/s11227-024-06443-9)].  [[Code](https://github.com/genglizhi/SiamMGT)] SIamMGT
* Specific and Collaborative Representations Siamese Network for RGBT Tracking. Yisong Liu , Dongming Zhou , Jinde Cao , Fellow, IEEE, Kaixiang Yan , and Lizhi Geng. IEEE SENSORS JOURNAL 2024. [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500316)].  [Code] SiamSCR
* Special attribute-based cross-modal interactive fusion network for RGBT tracking. XIAOQIANG SHAO, HAO LI, ZHIYUE LYU, BO MA, ZEHUI HAN AND MINGQIAN LIU. 2024. [[Paper](https://preprints.opticaopen.org/articles/preprint/Special_attribute-based_cross-modal_interactive_fusion_network_for_RGBT_tracking/2590421)] [Code]. ASFNet
* Simplifying Cross-modal Interaction via Modality-Shared Features for RGBT Tracking. LiQiu Chen, Yuqing Huang, Hengyu li, Zikun Zhou, Zhenyu He. ACMMM 2024. [[Paper](https://openreview.net/forum?id=oDEqOhKYoO)] [Code]. IIMF
* Top-down Cross-modal Guidance for Robust RGB-T Tracking. Liang Chen, Bineng Zhong‚àó, Qihua Liang, Yaozong Zheng, Zhiyi Mo, Shuxiang Song. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10614652)] [Code]. TGTrack
* Temporal Adaptive RGBT Tracking with Modality Prompt. Hongyu Wang, Xiaotao Liu*, Yifan Li, Meng Sun, Dian Yuan, Jing Liu. AAAI 2024. [[Paper](https://arxiv.org/abs/2401.01244)] [Code]. TATrack
* Towards Modalities Correlation for RGB-T Tracking. Hu Xiantao, Zhong Bineng, Liang Qihua, Zhang Shengping, Li Ning, Li Xianxian. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10517645)] [[Code](https://github.com/GXNU-ZhongLab/MCTrack)]. MCTrack.
* Transformer-based RGB-T Tracking with Channel and Spatial Feature Fusion. Yunfeng Li, Bo Wang, Ye Li, Zhiwen Yu, Liang Wang. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.03177)] [[Code](https://github.com/LiYunfengLYF/CSTNet)]. CSTNet.
* Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens. Dengdi Sun, Yajie Pan, Andong Lu, Chenglong Li, Bin Luo. Arxiv 2024. [[Paper](https://arxiv.org/abs/2401.01674)] [[Code]()]. STMT.
* Unified Single-Stage Transformer Network for Efficient RGB-T Tracking. Jianqiang Xia, DianXi Shi, Ke Song, Linna Song, XiaoLei Wang, Songchang Jin, Li Zhou, Yu Cheng, Lei Jin, Zheng Zhu, Jianan Li, Gang Wang, Junliang Xing, Jian Zhao. IJCAI 2024. [[paper](https://arxiv.org/abs/2308.13764)]  [[Code](https://github.com/xiajianqiang/USTrack)] USTrack


2023

* Anchor free based Siamese network tracker with transformer for RGB‚ÄëT tracking. Liangsong Fan, Pyeoungkee Kim. Scientific Reports 2023.  [[paper](https://www.nature.com/articles/s41598-023-39978-7)]  [Code] SiamAFTS
* An RGB-T Object Tracking Method for Solving Camera Motion Based on Correlation Filter. Zhongxuan Zhao, Weixing Li, Feng Pan. CCDC 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10326582)]  [Code] PRO
* Bayesian Dumbbell Diffusion Model for RGBT Object Tracking With Enriched Priors. Fan, Shenghua and He, Chu and Wei, Chenxia and Zheng, Yujin and Chen, Xi. SPL 2023. [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10184056)]  [Code] BD2Track
* Bridging Search Region Interaction with Template for RGB-T Tracking. Hui Tianrui, Xun Zizheng, Peng Fengguang, Huang Junshi, Wei Xiaoming, Wei Xiaolin, Dai Jiao, Han Jizhong, Liu Si. CVPR 2023. [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf)]  [[Code](https://github.com/RyanHTR/TBSI)] TBSI
* Dynamic Fusion Network for RGBT Tracking. Jingchao Peng , Haitao Zhao , and Zhengwei Hu. TITS 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/9997229)]  [[Code](https://github.com/PengJingchao/DFNet)]  DFNet.
* Deep Triply Attention Network for RGBT Tracking. Rui Yang, Xiao Wang, Yabin Zhu, Jin Tang. Cognitive Computation 2023. [[paper](https://link.springer.com/article/10.1007/s12559-023-10158-z)]  [Code] DTAN
* Differential Enhancement and Commonality Fusion for RGBT Tracking. Yang, Jianrong and Dong, Enzeng and Tong, Jigang and Yang, Sen and Zhang, Zufeng and Li, Wenyu. ICMA 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10215574)]  [Code] DECFNet
* Drone Based RGBT Tracking with Dual-Feature Aggregation Network. Zhinan Gao, Dongdong Li, Gongjian Wen, Yangliu Kuai, Rui Chen. Drones 2023. [[paper](https://www.mdpi.com/2504-446X/7/9/585)]  [Code] DBDFAN
* Dual-Modality Space-Time Memory Network for RGBT Tracking. Fan Zhang, Hanwei Peng, Lingli Yu, Yuqian Zhao, Baifan Chen. TIM 2023.  [[paper](https://ieeexplore.ieee.org/abstract/document/10143657)]  [Code]DMSTM
* Dynamic Tracking Aggregation with Transformers for RGB-T Tracking. X Liu, Z Lei. Journal of Information Processing Systems 2023. [[paper](https://jips-k.org/full-text/893)]  [Code] DTAT
* Efficient RGB-T Tracking via Cross-Modality Distillation. Zhang Tianlu, Guo Hongyuan, Jiao Qiang, Zhang Qiang, Han Jungong. CVPR 2023.  [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf)]  [Code] CMD.
* Exploring fusion strategies for accurate RGBT visual object tracking. Zhangyong Tang, Tianyang Xu, Hui Li, Xiao-Jun Wu, XueFeng Zhu, Josef Kittler. Information Fusion 2023. [[paper](https://www.sciencedirect.com/science/article/pii/S1566253523001975)]  [[Code](https://github.com/Zhangyong-Tang/DFAT-Information-Fusion2023)] DFAT
* EANet: Enhanced Attribute-Based RGBT Tracker Network. Abbas T√ºrkoƒülu, Erdem Akag√ºnd√ºz. ICMV 2023. [[paper](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/1307218/EANet-enhanced-attribute-based-RGBT-tracker-network/10.1117/12.3023347.full)]  [[Code](https://github.com/abbasturkoglu/EANet)] EANet
* Exploring the potential of Siamese network for RGBT object tracking. Liangliang Feng, Kechen Song, Junyi Wang, Yunhui Yan. JVCIR 2023.  [[paper](https://www.sciencedirect.com/science/article/pii/S1047320323001323)]  [Code] SiamFEA
* HATFNet: Hierarchical adaptive trident fusion network for RGBT tracking. Yanjie Zhao, Huicheng Lai & Guxue Gao. Applied Intelligence 2023. [[paper](https://link.springer.com/article/10.1007/s10489-023-04755-6)]  [Code] HATFNet
* Learning Multi-Layer Attention Aggregation Siamese Network for Robust RGBT Tracking. Mingzheng Feng and Jianbo Su. TMM 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10234663)] [Code]. SiamMLAA
* Learning Modality Complementary Features with Mixed Attention Mechanism for RGB-T Tracking. Yang Luo, Xiqing Guo, Mingtao Dong, Jin Yu. Sensors 2023. [[paper](https://www.mdpi.com/1424-8220/23/14/6609)]  [Code] MACFT
* Learning cross-modal interaction for RGB-T tracking. Chunyan XU, Zhen CUI*, Chaoqun WANG, Chuanwei ZHOU & Jian YANG. SCIENCE CHINA Information Sciences 2023. [[paper](http://scis.scichina.com/en/2023/119103-supplementary.pdf)]  [Code] LCMIT
* Learning modality feature fusion via transformer for RGBT-tracking. Yujue Cai, Xiubao Sui, Guohua Gu, Qian Chen. IPT 2023. [[paper](https://www.sciencedirect.com/science/article/pii/S1350449523002773)]  [[Code](https://github.com/caiyujue/MMMPT)] MMMPT
* Mask Refined Deep Fusion Network With Dynamic Memory for Robust RGBT Tracking. Ce Bian, Sei-ichiro Kamata. ICPR 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10331955)] [Code] ]MPDMT
* Multiple frequency‚Äìspatial network for RGBT tracking in the presence of motion blur. Shenghua Fan, Xi Chen, Chu He, Lei Yu, Zhongjie Mao, Yujin Zheng. Neural Computing and Applications 2023. [[paper](https://link.springer.com/article/10.1007/s00521-023-09024-8)]  [Code] FSBNet
* MTNet: Learning Modality-aware Representation with Transformer for RGBT Tracking. Ruichao Hou, Boyue Xu, Tongwei Ren, Gangshan Wu. ICME 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10219799)]  [[Code](https://github.com/xuboyue1999/MTNet-ICME23)] MTNet.
* Multi-modal multi-task feature fusion for RGBT tracking. Yujue Cai, Xiubao Sui ‚àó, Guohua Gu. INFFUS 2023.  [[paper](https://www.sciencedirect.com/science/article/pii/S1566253523001252)] [Code] JTPMA
* Multi-Modal Fusion Object Tracking Based on Fully Convolutional Siamese Network. Ke Qi, Liji Chen, Yicong Zhou, Yutao Qi. CACML 2023. [[paper](https://dl.acm.org/doi/abs/10.1145/3590003.3590084)]  [Code] SiamMFF.
* Online Learning Samples and Adaptive Recovery for Robust RGB-T Tracking. Jun Liu, Zhongqiang Luo, Xingzhong Xiong. TCSVT 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10159404)]  [Code] LSAR.
* Object Fusion Tracking for RGB-T Images via Channel Swapping and Modal Mutual Attention. Luan, Tian and Zhang, Hui and Li, Jiafeng and Zhang, Jing and Zhuo, Li. IEEE Sensors Journal 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10224831)]  [Code] CSMMA
* Quality-Aware RGBT Tracking via Supervised Reliability Learning and Weighted Residual Guidance. Lei Liu, Chenglong Li, Yun Xiao, Jin Tang. ACM MM 2023. [[paper](https://dl.acm.org/doi/abs/10.1145/3581783.3612341)]  [[Code](https://github.com/liulei970507/QAT)] QAT.
* ROBUST RGB-T TRACKING VIA CONSISTENCY REGULATED SCENE PERCEPTION. Bin Kang, Liwei Liu, Shihao Zhao, Songlin Du. ICIP 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10222904)]  [Code] CRSP
* RGB-T object tracking via sparse response-consistency discriminative correlation filters. Yueping Huang, Xiaofeng Li, Ruitao Lu, Naixin Qi. IPT 2023. [[paper](https://www.sciencedirect.com/science/article/pii/S135044952200490X)]  [Code] SRCDCF
* RGB-T Tracking via Multi-Modal Mutual Prompt Learning. Yang Luo, Xiqing Guo, Hui Feng, Lei Ao. Arxiv 2023. [[paper](https://arxiv.org/ftp/arxiv/papers/2308/2308.16386.pdf)]  [[Code](https://github.com/HusterYoung/MPLT)] MPLT. 
* Region Selective Fusion Network for Robust RGB-T Tracking. Yu, Zhencheng and Fan, Huijie and Wang, Qiang and Li, Ziwan and Tang, Yandong. SPL 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10252033)]  [Code] RSFNet
* Robust RGB-T Tracking via Graph Attention-Based Bilinear Pooling. Bin Kang, Dong Liang, Junxi Mei, Xiaoyang Tan, Quan Zhou, Dengyin Zhang. TNNLS 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/9756634)]  [Code] GABBP
* RGBT tracking based on prior least absolute shrinkage and selection operator and quality aware fusion of deep and handcrafted features. Seyed Morteza Ghazali, Yasser Baleghi. KBS 2023. [[paper](https://www.sciencedirect.com/science/article/pii/S0950705123004331)]  [[Code](https://github.com/mortezaghazali586/PLASSO-ADSPF-tracker)] PLASSO-ADSPF
* RGBT tracking based on modality feature enhancement. Sulan Zhai, Yi Wu1, Lei Liu, Jin Tang. Multimedia Tools and Applications 2023. [[paper](https://link.springer.com/article/10.1007/s11042-023-16418-2)]  [Code] MFENet
* RMFNet: Redetection Multimodal Fusion Network for RGBT Tracking. Yanjie Zhao, Huicheng Lai, and Guxue Gao. Applied Sciences 2023. [[paper](https://www.mdpi.com/2076-3417/13/9/5793)]  [Code] RMFNet
* RGBT Tracking via Multi-stage Matching Guidance and Context integration. Kaixiang Yan, Changcheng Wang, Dongming Zhou, Ziwei Zhou. Neural Processing Letters 2023. [[paper](https://link.springer.com/article/10.1007/s11063-023-11365-3)]  [Code] M2GCI
* Siamese infrared and visible light fusion network for RGB-T tracking. Jingchao Peng, Haitao Zhao, Zhengwei Hu, Yi Zhuang, Bofan Wang. Journal of Machine Learning and Cybernetics 2023. [[paper](https://link.springer.com/article/10.1007/s13042-023-01833-6)]  [[Code](https://github.com/PengJingchao/SiamIVFN)] SiamIVFN
* SiamTDR: Time-Efficient RGBT Tracking via Disentangled Representations. Guorui Wang , Qian Jiang , Xin Jin , Member, IEEE, Yu Lin, Yuanyu Wang , and Wei Zhou. TICPS 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10226308)]  [Code] SiamTDR
* Siamese transformer RGBT tracking. Futian Wang, Wenqi Wang, Lei Liu, Chenglong Li & Jing Tang. Applied Intelligence 2023. [[paper](https://link.springer.com/article/10.1007/s10489-023-04741-y)]  [Code] STRT
* Semantic-guided fusion for multiple object tracking and RGB-Ttracking. Xiaohu Liu, Yichuang Luo, Yan Zhang, Zhiyong Lei. IET Image Processing 2023. [[paper](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12861)]  [Code] SGF-MDNet+RGBT
* SiamCAF: Complementary Attention Fusion-Based Siamese Network for RGBT Tracking. Yingjian Xue, Jianwei Zhang, Zhoujin Lin, Chenglong Li, Bihan Huo, and Yan Zhang. Remote Sensing 2023. [[paper](https://www.mdpi.com/2072-4292/15/13/3252)]  [Code] SiamCAF
* TEFNet: Target-Aware Enhanced Fusion Network for RGB-T Tracking. Panfeng Chen, Shengrong Gong, Wenhao Ying, Xin Du & Shan Zhong. PRCV 2023. [[paper](https://link.springer.com/chapter/10.1007/978-981-99-8549-4_36)]  [Code] TEFNet.
* Thermal infrared and visible sequences tracking via dual adversarial pixel fusion. Hang Zheng, Nangezi Yuan, Hongwei Ding, Peng Hu & Zhijun Yang. Multimedia Tools and Applications 2023. [[paper](https://link.springer.com/article/10.1007/s11042-023-17721-8)]  [Code] DAPF
* Unsupervised RGB-T object tracking with attentional multi-modal feature fusion. Shenglan Li, Rui Yao, Yong Zhou, Hancheng Zhu, Bing Liu, Jiaqi Zhao & Zhiwen Shao.  Multimedia Tools and Applications  2023. [[paper](https://link.springer.com/article/10.1007/s11042-023-14362-9)]  [Code] UDT-FF
* Unveiling the Power of Unpaired Multi-modal Data for RGBT Tracking. Qing Shen, Yifan Wang, Yu Guoa and Mengmeng Yang. International Conference on Artificial Intelligence and Electromechanical Automation 2023. [[paper](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12709/127092N/Unveiling-the-power-of-unpaired-multi-modal-data-for-RGBT/10.1117/12.2685082.full)]  [Code] UMT.
* Visible‚ÄìInfrared Dual-Sensor Fusion for Single-Object Tracking. Weichun Liu , Weibing Liu, and Yuxin Sun.  IEEE SENSORS JOURNAL 2023. [[paper](https://ieeexplore.ieee.org/abstract/document/10012641)]  [Code] CSRDCF_RGBT


2022
* Asymmetric Global‚ÄìLocal Mutual Integration Network for RGBT Tracking. Mei Jiatian, Liu Yanyu, Wang Changcheng, Zhou Dongming, Nie Rencan, Cao Jinde. TIM 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9840392)]  [Code] AGMINet.
* Attribute-Based Progressive Fusion Network for RGBT Tracking. Yun Xiao, Mengmeng Yang, Chenglong Li, Lei Liu, Jin Tang. AAAI 2022.[[paper](https://ojs.aaai.org/index.php/AAAI/article/view/20187)]  [[Code](https://github.com/yangmengmeng1997/APFNet)] APFNet
* Correlation Filters Based on Strong Spatio-Temporal for Robust RGB-T Tracking. Futing Luo, Mingliang Zhou, and Bing Fang. Journal of Circuits, Systems and Computers 2022. [[paper](https://www.worldscientific.com/doi/abs/10.1142/S0218126622500414)]  [Code] can not download
* CMC2R: Cross-modal collaborative contextual representation for RGBT tracking. Xiaohu Liu, Yichuang Luo, Keding Yan, Jianfei Chen, Zhiyong Lei. IET Image Processing 2022. [[paper](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12427)]  [Code] CMC2R
* Duality-Gated Mutual Condition Network for RGBT Tracking. Lu Andong, Qian Cun, Li Chenglong, Tang Jin, Wang Liang. TNNLS 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9737634)]  [[Code](https://github.com/Alexadlu/DMCNet)] DMCNet.
* Dual Siamese network for RGBT tracking via fusing predicted position maps. Chang Guo, Dedong Yang, Chang Li, Peng Song. The Visual Computer 2022.[[paper](https://link.springer.com/article/10.1007/s00371-021-02131-4)]  [Code] DuSIamRT
* HIGH SPEED AND ROBUST RGB-THERMAL TRACKING VIA DUAL ATTENTIVE STREAM SIAMESE NETWORK. Guo Chaoyang, Xiao Liang. IGARSS 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9883659)]  [[Code](https://github.com/easycodesniper-afk/SiamCSR)] SiamCSR
* Learning reliable modal weight with transformer for robust RGBT tracking. Mingzheng Feng, Jianbo Su. KBS 2022. [[paper](https://www.sciencedirect.com/science/article/pii/S0950705122004579)]  [Code] LRMWT
* MIRNET: A ROBUST RGBT TRACKING JOINTLY WITH MULTI-MODAL INTERACTION AND REFINEMENT. Ruichao Hou, Tongwei Ren , Gangshan Wu. ICME 2022.[[paper](https://ieeexplore.ieee.org/abstract/document/9860018)]  [[Code](https://github.com/xuboyue1999/MIRNet-ICME22)] MIRNet
* Multibranch Adaptive Fusion Network for RGBT Tracking. Li Yadong, Lai Huicheng, Wang Liejun, Jia Zhenhong. IEEE Sensors Journal 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9721310)]  [Code] MBAFNet.
* MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking. Xiao Wang, Xiujun Shu, Shiliang Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu. TMM 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9772993)]  [[Code](https://github.com/wangxiao5791509/MFG_RGBT_Tracking_PyTorch)] MFGNet.
* RGBT tracking via reliable feature configuration. Zhengzheng Tu, Wenli Pan, Yunsheng Duan, Jin Tang & Chenglong Li.  Science China Information Sciences 2022. [[paper](https://link.springer.com/article/10.1007/s11432-020-3160-5)]  [Code] RFC
* RGB-T long-term tracking algorithm via local sampling and global proposals. Liu Jun, Luo Zhongqiang, Xiong Xingzhong. Signal, Image and Video Processing 2022. [[paper](https://link.springer.com/article/10.1007/s11760-022-02187-2)]  [Code] CF-LG
* RGB-T tracking by modality difference reduction and feature re-selection. Qiang Zhang, Xueru Liu, Tianlu Zhang. IVC 2022. [[paper](https://www.sciencedirect.com/science/article/pii/S0262885622001767)]  [Code] MFNet
* RGBT Tracking by Trident Fusion Network. Zhu Yabin and Li, Chenglong and Tang, Jin and Luo, Bin and Wang, Liang. TCSVT 2022. [[paper](https://ieeexplore.ieee.org/abstract/document/9383014)]  [Code] TFNet.
* Residual learning-based two-stream network for RGB-T object tracking. Yili Chen, Minjie Wan, Yunkai Xu, Xiaojie Zhang, Qian Chen, Guohua Gu. JEI 2022. [[paper](https://scholar.google.com/scholar?q=Residual+learning-based+two-stream+network+for+RGB-T+object+tracking&hl=zh-CN&as_sdt=0,5)]  [[Code](https://github.com/MinjieWan/Residual-learning-based-two-stream-network-for-RGB-T-object-tracking)] RLTN
* SCA-MMA: Spatial and Channel-Aware Multi-Modal Adaptation for Robust RGB-T Object Tracking. Run Shi, Chaoqun Wang, Gang Zhao, Chunyan Xu. ELECTRONICS 2022. [[paper](https://www.mdpi.com/2079-9292/11/12/1820)]  [Code] SCA-MMA
* SiamMMF: multi-modal multi-level fusion object tracking based on Siamese networks. Zhen Yang, Peng Huang, Dunyun He, Zhongwang Cai & Zhijian Yin. Machine Vision and Applications 2022. [[paper](https://link.springer.com/article/10.1007/s00138-022-01354-2)]  [Code] SiamMMF
* Temporal Aggregation for Adaptive RGBT Tracking. Tang, Zhangyong and Xu, Tianyang and Wu, Xiao-Jun. Arxiv 2022. [[paper](https://arxiv.org/abs/2201.08949)]  [[Code](https://github.com/Zhangyong-Tang/TAAT)] TAAT
* Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline. Pengyu Zhang, Jie Zhao, Dong Wang, Huchuan Lu, Xiang Ruan. CVPR 2022. [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Visible-Thermal_UAV_Tracking_A_Large-Scale_Benchmark_and_New_Baseline_CVPR_2022_paper.html)]  [[Code](https://github.com/zhang-pengyu/HMFT)] HMFT.


2021

* Adaptive Fusion CNN Features for RGBT Object Tracking. Wang, Yong and Wei, Xian and Tang, Xuan and Shen, Hao and Zhang, Huanlong. TITS 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9426573)]  [Code] AFCF
* Channel Exchanging for RGB-T Tracking. Long Zhao, Meng Zhu, Honge Ren,  Lingjixuan Xue. Sensors 2021.[[paper](https://www.mdpi.com/1424-8220/21/17/5800)]  [Code] CEDiMP
* Enhanced Real-Time RGB-T Tracking by Complementary Learners. Qingyu Xu, Yangliu Kuai, Junggang Yang, and Xinpu Deng. Journal of Circuits, Systems and Computers 2021. [[paper](https://www.worldscientific.com/doi/abs/10.1142/S0218126621503072)]  [Code]EStaple
* HDINet: Hierarchical Dual-Sensor Interaction Network for RGBT Tracking. Mei, Jiatian and Zhou, Dongming and Cao, Jinde and Nie, Rencan and Guo, Yanbu. IEEE Sensors Journal 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9426927)]  [Code]HDINet
* Jointly Modeling Motion and Appearance Cues for Robust RGB-T Tracking. Zhang, Pengyu and Zhao, Jie and Bo, Chunjuan and Wang, Dong and Lu, Huchuan and Yang, Xiaoyun. TIP 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9364880)]  [[Code](https://github.com/zhang-pengyu/JMMAC)] JMMAC.
* Learning Adaptive Attribute-Driven Representation for Real-Time RGB-T Tracking. Pengyu Zhang, Dong Wang, Huchuan Lu, Xiaoyun Yang. IJCV 2021. [[paper](https://link.springer.com/article/10.1007/s11263-021-01495-3)]  [[Code](https://github.com/zhang-pengyu/ADRNet)] ADRNet. 
* Learning a Twofold Siamese Network for RGB-T Object Tracking. Yangliu Kuai, Dongdong Li, and Que Qian. Journal of Circuits, Systems and Computers 2021. [[paper](https://www.worldscientific.com/doi/abs/10.1142/S0218126621500894)]  [Code] can not download
* M5L: Multi-Modal Multi-Margin Metric Learning for RGBT Tracking. Zhengzheng Tu, Chun Lin, Chenglong Li, Jin Tang, Bin Luo. TIP 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9617143)]  [Code] M5L
* Multimodal Cross-Layer Bilinear Pooling for RGBT Tracking. Qin Xu, Yiming Mei, Jinpei Liu, and Chenglong Li. TMM 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9340007)]  [Code] CBPNet
* Quality-Aware Feature Aggregation Network for Robust RGBT Tracking. Yabin Zhu , Chenglong Li , Jin Tang , and Bin Luo. TIV 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9035457)]  [Code]  FANet 
* RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. Andong Lu, Chenglong Li, Yuqing Yan, Jin Tang, Bin Luo. TIP 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9454275)]  [[Code](https://github.com/Alexadlu/MANet_pp)] MANet++
* RGBT Tracking via Noise-Robust Cross-Modal Ranking. Li, Chenglong and Xiang, Zhiqiang and Tang, Jin and Luo, Bin and Wang, Futian. TNNLS 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9406193)]  [[Code](https://chenglongli.cn/code-dataset/)] NRCMR
* RGBT tracking via cross-modality message passing. Rui Yang, Xiao Wang, Chenglong Li, Jinmin Hu, Jin Tang. Neurocomputing 2021.[[paper](https://www.sciencedirect.com/science/article/pii/S0925231221011966)]  [Code] CMMP
* SiamCDA: Complementarity-and distractor-aware RGB-T tracking based on Siamese network. Zhang, Tianlu and Liu, Xueru and Zhang, Qiang and Han, Jungong. TCSVT 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9399460)]  [[Code](https://github.com/Tianlu-Zhang/LSS-Dataset)] SiamCDA.
* WF_DiMP: weight-aware dual-modal feature aggregation mechanism for RGB-T tracking. Zhaodi Wang, Yan Ding, Pingping Wu, Jinbo Zhang. SEVENTH SYMPOSIUM ON NOVEL PHOTOELECTRONIC DETECTION TECHNOLOGY AND APPLICATIONS 2021. [[paper](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11763/1176362/WF_DiMP--weight-aware-dual-modal-feature-aggregation-mechanism-for/10.1117/12.2587295.full)]  [Code] WF_DiMP


2020

* Cross-Modal Pattern-Propagation for RGB-T Tracking. Chaoqun Wang, Chunyan Xu, Zhen Cui, Ling Zhou, Tong Zhang, Xiaoya Zhang, Jian Yang. CVPR 2020.[[paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Cross-Modal_Pattern-Propagation_for_RGB-T_Tracking_CVPR_2020_paper.html)]  [Code] CMPP
* Challenge-Aware RGBT Tracking. Chenglong Li, Lei Liu, Andong Lu, Qing Ji, Jin Tang. ECCV 2020. [[paper](https://link.springer.com/chapter/10.1007/978-3-030-58542-6_14)]  [[Code](https://github.com/liulei970507/CAT)] CAT
* DSiamMFT: An RGB-T fusion tracking method via dynamic Siamese networks using multi-layer feature fusion. Xingchen Zhang, Ping Ye, Shengyun Peng, Jun Liu, Gang Xiao. Signal Processing: Image Communication 2020. [[paper](https://www.sciencedirect.com/science/article/pii/S092359651930342X)]  [Code] DSiamMFT
* Learning discriminative update adaptive spatial-temporal regularized correlation filter for RGB-T tracking. Mingzheng Feng, Kechen Song, Yanyan Wang, Jie Liu, Yunhui Yan. Journal of Visual Communication and Image Representation 2020. [[paper](https://www.sciencedirect.com/science/article/pii/S1047320320301279)]  [Code] LDUA-STRCF
* Object Tracking in RGB-T Videos Using Modal-Aware Attention Network and Competitive Learning. Hui Zhang, Lei Zhang, Li Zhuo, Jing Zhang. Sensors 2020. [[paper](https://www.mdpi.com/1424-8220/20/2/393)]  [[Code](https://github.com/Lee-zl/MaCNet)] MacNet
* RGB-T Tracking via Multi-expert Correlation Filters using Spatial-temporal Robustness. Zhang, Fei and Ma, Shiping and Li, Zhijun and Zhang, Yule. ICMCCE 2020. [[paper](https://ieeexplore.ieee.org/abstract/document/9421777)]  [Code] MECF
* Robust RGB-T tracking via Bimodal Constrained Correlation Filtering. Li, Xin and Cai, Jun and Ding, Wan and Fang, Zhi. PIC 2020.  [[paper](https://ieeexplore.ieee.org/abstract/document/9350798)]  [Code] BCCF

2019

* Dense Feature Aggregation and Pruning for RGBT Tracking. Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, Xiao Wang. ACM MM 2019. [[paper](https://dl.acm.org/doi/abs/10.1145/3343031.3350928)]  [Code] DAPNet.
* Deep Adaptive Fusion Network for High Performance RGBT Tracking. Yuan Gao, Chenglong Li, Yabin Zhu, Jin Tang, Tao He, Futian Wang. ICCVW 2019. [[paper](https://openaccess.thecvf.com/content_ICCVW_2019/html/VISDrone/Gao_Deep_Adaptive_Fusion_Network_for_High_Performance_RGBT_Tracking_ICCVW_2019_paper.html)]  [[Code](https://github.com/mjt1312/DAFNet)] DAFNet
* Fast RGB-T Tracking via Cross- Modal Correlation Filters. Sulan Zhai, Pengpeng Shao*, Xinyan Liang, Xin Wang. Neurocomputing 2019. [[paper](https://www.sciencedirect.com/science/article/pii/S0925231219300347)]  [Code] CMCFT
* Learning Local-Global Multi-Graph Descriptors for RGB-T Object Tracking. Chenglong Li, Chengli Zhu, Jian Zhang, Bin Luo, Xiaohao Wu, and Jin Tang. TCSVT 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/8485393)]  [Code] LGMG
* Learning Target-oriented Dual Attention for Robust RGB-T Tracking. Rui Yang, Yabin Zhu, Xiao Wang, Chenglong Li, Jin Tang. Arxiv 2019.[[paper](https://arxiv.org/abs/1908.04441)]  [Code] LTODA
* Multi-Modal Fusion for End-to-End RGB-T Tracking. Lichao Zhang, Martin Danelljan, Abel Gonzalez-Garcia, Joost van de Weijer, Fahad Shahbaz Khan. ICCVW 2019. [[paper](https://openaccess.thecvf.com/content_ICCVW_2019/html/VOT/Zhang_Multi-Modal_Fusion_for_End-to-End_RGB-T_Tracking_ICCVW_2019_paper.html)]  [[Code](https://github.com/zhanglichao/end2end_rgbt_tracking)] mfDiMP
* Multi-Adapter RGBT Tracking. Chenglong Li, Andong Lu, Aihua Zheng, Zhengzheng Tu, Jin Tang. ICCVW 2019. [[paper](https://openaccess.thecvf.com/content_ICCVW_2019/html/VOT/Li_Multi-Adapter_RGBT_Tracking_ICCVW_2019_paper.html)]  [[Code](https://github.com/Alexadlu/MANet)] MANet 
* SiamFT: An RGB-Infrared Fusion Tracking Method via Fully Convolutional Siamese Networks. XINGCHEN ZHANG, PING YE, SHENGYUN PENG, JUN LIU, KE GONG1, GANG XIAO. IEEE Access 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/8809774)]  [Code] SiamFT
* Thermal infrared and visible sequences fusion tracking based on a hybrid tracking framework with adaptive weighting scheme. Chengwei Luoa, Bin Suna, Ke Yanga, Taoran Lua, Wei-Chang Yeh. IPT 2019. [[paper](https://www.sciencedirect.com/science/article/pii/S1350449519300258)]  [Code] AWS

2018

* Cross-Modal Ranking with Soft Consistency and Noisy Labels for Robust RGB-T Tracking. Chenglong Li, Chengli Zhu, Yan Huang, Jin Tang, Liang Wang. ECCV 2018.  [[paper](https://openaccess.thecvf.com/content_ECCV_2018/html/Chenglong_Li_Cross-Modal_Ranking_with_ECCV_2018_paper.html)]  [[Code](https://chenglongli.cn/code-dataset/)]
* Fusing two-stream convolutional neural networks for RGB-T object tracking. Chenglong Li, Xiaohao Wu, Nan Zhao, Xiaochun Cao, Jin Tang. Neurocomputing 2018. [[Paper](https://www.sciencedirect.com/science/article/pii/S0925231217318271)] [Code]. fusionnet.
* Learning Soft-Consistent Correlation Filters for RGB-T Object Tracking. Yulong Wang, Chenglong Li & Jin Tang. PRCV 2018. [[paper](https://link.springer.com/chapter/10.1007/978-3-030-03341-5_25)]  [Code] SCCF
* Learning Multi-domain Convolutional Network for RGB-T Visual Tracking. Xingming Zhang, Xuehan Zhang, Xuedan Du, Xiangming Zhou, Jun Yin. CISP 2018.  [[paper](https://ieeexplore.ieee.org/abstract/document/8633180)]  [Code] LMDCN
* Robust Collaborative Discriminative Learning for RGB-Infrared Tracking. Xiangyuan Lan, Mang Ye, Shengping Zhang, Pong C. Yuen. AAAI 2018. [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/12307)]  [Code] RCDL
* Two-stage modality-graphs regularized manifold ranking for RGB-T tracking. Chenglong Li, Chengli Zhu, Shaofei Zheng, Bin Luo, Jing Tang. Signal Processing: Image Communication 2018.  [[paper](https://www.sciencedirect.com/science/article/pii/S0923596518304892)]  [Code] TMGRMR

2017

* Grayscale-Thermal Object Tracking via Multitask Laplacian Sparse Representation. Chenglong Li, Xiang Sun, Xiao Wang, Lei Zhang, and Jin Tang. TSMCS 2017. [[paper](https://ieeexplore.ieee.org/abstract/document/7822984)]  [Code] MLSR



2016

* Real-Time Grayscale-Thermal Tracking via Laplacian Sparse Representation. Chenglong Li, Shiyi Hu, Sihan Gao, and Jin Tang. MultiMedia Modeling 2016. [[paper](https://link.springer.com/chapter/10.1007/978-3-319-27674-8_6)]  [Code] 

2012

* Fusion tracking in color and infrared images using joint sparse representation. LIU HuaPing, SUN FuChun. Science China Information Sciences 2012. [[paper](https://link.springer.com/article/10.1007/s11432-011-4536-9)]  [Code] JSR 


2011

* Multiple Source Data Fusion via Sparse Representation for Robust Visual Tracking. Wu, Yi and Blasch, Erik and Chen, Genshe and Bai, Li and Ling, Haibin. ICIF 2011. [[paper](https://ieeexplore.ieee.org/abstract/document/5977451)]  [Code] L1-PF


2008

* Thermo-visual feature fusion for object tracking using multiple spatiogram trackers. Conaire C √ì, O‚ÄôConnor N E, Smeaton A. Machine Vision and Applications 2008. [[paper](https://link.springer.com/article/10.1007/s00138-007-0078-y)]  [Code]


2007

* The Effect of Pixel-Level Fusion on Object Tracking in Multi-Sensor Surveillance Video. N. Cvejic, S. G. Nikolov, H. D. Knowles, A. ≈Åoza, A. Achim, D. R. Bull and C. N. Canagarajah. CVPR 2007. [[paper](https://ieeexplore.ieee.org/abstract/document/4270431)]  [Code]


2006

* Comparison of fusion methods for thermo-visual surveillance tracking. Conaire, C.O. and O'Connor, N.E. and Cooke, E. and Smeaton, A.F. ICIF 2006. [[paper](https://ieeexplore.ieee.org/abstract/document/4085904)]  [[Code]()]
* The influence of multi-sensor video fusion on object tracking using a particle filter. Mihaylova L., Loza A., Nikolov S. G., Lewis J. J., Canga E. -F., Li, J., Dixon T., Canagarajah C. N., Bull D. R. INFORMATIK 2006 [[paper](https://dl.gi.de/items/016d794a-8c54-4445-8eae-f8c78e5283eb)]  [Code]



### RGB-D Tracking

2024
* Adaptive Colour-Depth Aware Attention for RGB-D Object Tracking. Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu, Zhenhua Feng, Josef Kittler. SPL 2024. [[paper](https://ieeexplore.ieee.org/abstract/document/10472092)] [[Code](https://github.com/xuefeng-zhu5/CDAAT)] CDAAT
* DepthRefiner: Adapting RGB Trackers to RGBD Scenes via Depth-Fused Refinement. Simiao Lai, Dong Wang, Huchuan Lu. ICME 2024. [[paper]()] [Code] DepthRefiner
* Feature enhancement and coarse-to-fine detection for RGB-D tracking. Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu, Josef Kittler. PRL 2024. [[paper](https://www.sciencedirect.com/science/article/pii/S0167865524000412)] [[Code]()] FECD
* Self-supervised learning for RGB-D object tracking. Xue-Feng Zhu, Tianyang Xu, Sara Atito, Muhammad Awais, Xiao-Jun Wu, Zhenhua Feng, Josef Kittler. PR 2024. [[paper](https://www.sciencedirect.com/science/article/pii/S0031320324002942)] [Code] SSLTrack
* Temporal adaptive bidirectional bridging for RGB-D tracking. Ge Ying, Dawei Zhang, Zhou Ou, Xiao Wang, Zhonglong Zheng. PR 2024. [[paper](https://www.sciencedirect.com/science/article/pii/S0031320324008045?casa_token=6P-08tHJ5pkAAAAA:j7qF9oYkP_iVx3hHoNsx_nWTxaANWGXEUtkGOvIpw44txccFlaSYg-hl3z_Ux1u00izn4Yl2wA)] [Code] TABBTrack
* UBPT: Uni-directional and Bi-directional Prompts for RGBD Tracking. Zhou Ou, Dawei Zhang, Ge Ying, Zhonglong Zheng. IEEE Sensor Journal 2024. [[paper](https://ieeexplore.ieee.org/abstract/document/10706817?casa_token=EnGkP66wtkQAAAAA:X2Peu8zzgSmkiC41i8H78sLFfNoLVMytcOtNsc7qA9_3hDz87CMsiY3y_kBeIN4VpYbOUEtt9oY)] [Code] UBPT
* Visual Adapt for RGBD Tracking. Zhang, Guangtong and Liang, Qihua and Mo, Zhiyi and Li, Ning and Zhong, Bineng. ICASSP 2024. [[paper](https://ieeexplore.ieee.org/abstract/document/10447728)] [Code] VADT.
* 

2023 
* Resource-Effcient RGBD Aerial Tracking. Yang, Jinyu and Gao, Shang and Li, Zhe and Zheng, Feng and Leonardis, Ale\v{s}. CVPR 2023. [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Resource-Efficient_RGBD_Aerial_Tracking_CVPR_2023_paper.pdf)] [[Code](https://github.com/yjybuaa/RGBDAerialTracking)] EMT
* RGBD1K: A Large-Scale Dataset and Benchmark for RGB-D Object Tracking. Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, ZuchengWu, Haodong Liu, Xiao Yang, Xiao-Jun Wu1*, Josef Kittler. AAAI 2023. [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25500)] [[Code](https://github.com/xuefeng-zhu5/SPT)] SPT
* RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network. Boyue Xu, Yi Xu, Ruichao Hou, Jia Bei, Tongwei Ren, Gangshan Wu. ACM MMA 2023. [[paper](https://dl.acm.org/doi/abs/10.1145/3595916.3626441)] [Code] HMAD

2022
* Learning Dual-Fused Modality-Aware Representations for RGBD Tracking. Shang Gao, Jinyu Yang, Zhe Li, Feng Zheng, Ale≈° Leonardis, Jingkuan Song. ECCVW 2022. [[paper](https://link.springer.com/chapter/10.1007/978-3-031-25085-9_27)] [[Code](https://github.com/ShangGaoG/DMTracker)] DMTracker
* 

2021
* DepthTrack: Unveiling the Power of RGBD Tracking. Song Yan, Jinyu Yang, Jani K¬®apyl¬®a, Feng Zheng, AleÀás Leonardis, Joni-Kristian K¬®am¬®ar¬®ainen. ICCV 2021. [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_DepthTrack_Unveiling_the_Power_of_RGBD_Tracking_ICCV_2021_paper.pdf)] [[Code](https://github.com/xiaozai/DeT)] DeT.
* DAL : A deep depth-aware long-term tracker. Yanlin Qian, Alan Lukezic, Matej Kristan, Joni-Kristian K√§m√§r√§inen, Jiri Matas. ICPR 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9412984)] [Code] DAL
* Single-scale siamese network based RGB-D object tracking with adaptive bounding boxes. Feng Xiao, Qiuxia Wu, Han Huang. Neurocomputing 2021. [[paper](https://www.sciencedirect.com/science/article/pii/S0925231221005439)] [Code] 3s-RGBD
* TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator. Pengyao Zhao, Quanli Liu, Wei Wang and Qiang Guo. ICPR 2021. [[paper](https://ieeexplore.ieee.org/abstract/document/9413315)] [[Code](https://github.com/lql-team/TSDM)] TSDM
* 

2020
* An Occlusion-Aware RGB-D Visual Object Tracking Method Based on Siamese Network. Wenli Zhang, Kun Yang, Yitao Xin, Rui Meng. ICSP 2020. [[paper](https://ieeexplore.ieee.org/abstract/document/9320907)] [Code] SiamOC
* SRDT: A Novel Robust RGB-D Tracker Based on Siamese Region Proposal Network and Depth Information. Zhen Sun, Junfei Wu, Lu Wang, and Qingdang Li. International Journal of Pattern Recognition and Artificial Intelligence 2020. [[paper](https://www.worldscientific.com/doi/abs/10.1142/S0218001420540233)] [Code] SRDT can not download
* Robust fusion for RGB-D tracking using CNN features. Yong Wang, Xian Wei, Hao Shen, Lu Ding, Jiuqing Wan. Applied Soft Computing Journal 2020. [[paper](https://www.sciencedirect.com/science/article/pii/S1568494620302428)] [Code] RF-CFF.
* Robust RGBD Tracking via Weighted Convlution Operators. Weichun Liu, Xiaoan Tang, Chengling Zhao. IEEE Sensors Journal 2020. [[paper](https://ieeexplore.ieee.org/abstract/document/8950173)] [Code] WCO
* Robust RGB-D tracking via compact CNN features. Yong Wang, Xian Wei, Lingkun Luo, Wen Wen, Yang Wang. Engineering Applications of Artificial Intelligence 2020. [[paper](https://www.sciencedirect.com/science/article/pii/S0952197620302803)] [Code] CF-RGBD

2019
* CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark. Alan LukeÀáziÀác, Ugur Kart, Jani K¬®apyl¬®a, Ahmed Durmush, Joni-Kristian K¬®am¬®ar¬®ainen, JiÀár¬¥ƒ± Matas and Matej Kristan. ICCV 2019. [[paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Lukezic_CDTB_A_Color_and_Depth_Visual_Object_Tracking_Dataset_and_ICCV_2019_paper.html)] [Code] CDTB.
* Context-Aware Three-Dimensional Mean-Shift With Occlusion Handling for Robust Object Tracking in RGB-D Videos. Ye Liu, Xiao-Yuan Jing, Jianhui Nie, Hao Gao, Jun Liu, Guo-Ping Jiang. TMM 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/8425768)] [Code] CA3DMS
* Depth Information Aided Constrained correlation Filter for Visual Tracking. Guanqun Li, Lei Huang, Peichang Zhang, Qiang Li, YongKai Huo. GSKI  2019. [[paper](https://iopscience.iop.org/article/10.1088/1755-1315/234/1/012005/meta)] [Code] Depth-CCF
* DS-KCF: A Real-time Tracker for RGB-D Data. Sion Hannuna, Massimo Camplani, Jake Hall, Majid Mirmehdi, Dima Damen, Tilo Burghardt, Adeline Paiement, Lili Tao. RTIP 2019. [[paper](https://link.springer.com/article/10.1007/s11554-016-0654-3)] [Code] DS-KCF_shape
* Hierarchical multi-modal fusion FCN with attention model for RGB-D tracking. Ming-xin Jiang, Chao Deng, Jing-song Shan, Yuan-yuan Wang, Yin-jie Jia, Xing Sun. Information Fusion 2019. [[paper](https://www.sciencedirect.com/science/article/pii/S1566253517306784)] [Code] H-FCN
* Object Tracking by Reconstruction with View-Specific Discriminative Correlation Filters. Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas. CVPR 20219. [[paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Kart_Object_Tracking_by_Reconstruction_With_View-Specific_Discriminative_Correlation_Filters_CVPR_2019_paper.html)] [[Code](https://github.com/ugurkart/OTR)] OTR
* RGB-D Object Tracking with Occlusion Detection. Yujun Xie, Yao Lu, Shuang Gu. CIS 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/9023755)] [Code] RGBD-OD
* RGB-D tracker under Hierarchical structure. Li, Yifan and Wang, Xuan and Jiang, Zoe L. and Qi, Shuhan and Liu, Xinhui and Chen, Qian. CIFEr 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/8759064)] [Code] HST
* Target-Aware Correlation Filter Tracking in RGBD Videos. Kuai, Yangliu and Wen, Gongjian and Li, Dongdong and Xiao, Jingjing. IEEE Sensors Journal 2019. [[paper](https://ieeexplore.ieee.org/abstract/document/8752050)] [Code] ECO_TA
* Visual Object Tracking in RGB-D Data via Genetic Feature Learning. Ming-xin Jiang, Xian-xian Luo, Tao Hai, Hai-yan Wang, Song Yang and Ahmed N. Abdalla. Complexity 2019. [[paper](https://onlinelibrary.wiley.com/doi/full/10.1155/2019/4539410)] [Code] GFL

2018
* A Real-time RGB-D tracker based on KCF. Han Zhang, Meng Cai, Jianxun Li. CCDC 2018. [[paper](https://ieeexplore.ieee.org/abstract/document/8407972)] [Code] RT-KCF
* Depth Masked Discriminative Correlation Filter. Uƒüur Kart, Joni-Kristian K√§m√§r√§inen, Ji≈ô√≠ Matas, Lixin Fan, Francesco Cricri. ICPR 2018. [[paper](https://ieeexplore.ieee.org/abstract/document/8546179)] [Code] DM-DCF
* How to Make an RGBD Tracker ?. Kart, Uƒüur and K√§m√§r√§inen, Joni-Kristian and Matas, Ji≈ô√≠. ECCVW 2018. [[paper](https://openaccess.thecvf.com/content_eccv_2018_workshops/w1/html/Kart_How_to_Make_an_RGBD_Tracker__ECCVW_2018_paper.html)] [[Code](https://github.com/ugurkart/rgbdconverter)] CSRDCF_RGBD++
* Multimodal Deep Feature Fusion (MMDFF) for RGB-D Tracking. Ming-xin Jiang, Chao Deng, Ming-min Zhang, Jing-song Shan, and Haiyan Zhang. Complexity 2018. [[paper](https://onlinelibrary.wiley.com/doi/full/10.1155/2018/5676095)] [Code] MMDFF
* Occlusion-Aware Correlation Particle Filter Target Tracking Based on RGBD Data. Yayu Zhai, Ping Song, Zonglei Mou, Xiaoxiao Chen, Xiongjun Liu. IEEE Access 2018. [[paper](https://ieeexplore.ieee.org/abstract/document/8463446)] [Code] OACPF
* Robust Fusion of Color and Depth Data for RGB-D Target Tracking Using Adaptive Range-Invariant Depth Models and Spatio-Temporal Consistency Constraints. Jingjing Xiao, Rustam Stolkin, Yuqing Gao, and Ale≈° Leonardis. TCYB 2018. [[paper](https://ieeexplore.ieee.org/abstract/document/8026575)] [Code] STC.
* Real-Time RGB-D Visual Tracking With Scale Estimation and Occlusion Handling. Jiaxu Leng, Ying Liu. IEEE Access 2018. [[paper](https://ieeexplore.ieee.org/abstract/document/8353501)] [Code] SEOH
  

2017
* Robust Object Tracking with RGBD-based Sparse Learning. Zi-ang Ma, Zhi-yu Xiang. ITEE 2017. [[paper](https://link.springer.com/article/10.1631/FITEE.1601338)] [Code] ROTSL
* RGB-D Tracking Based on Kernelized Correlation Filter with Deep Features. Gu, Shuang and Lu, Yao and Zhang, Lin and Zhang, Jian. ICONIP 2017. [[paper](https://link.springer.com/chapter/10.1007/978-3-319-70090-8_11)] [Code] KCFDF
* Visual Object Tracking Based on Cross-Modality Gaussian-Bernoulli Deep Boltzmann Machines with RGB-D Sensors. Mingxin Jiang, Zhigeng Pan and Zhenzhou Tang. Sensors 2017. [[paper](https://www.mdpi.com/1424-8220/17/1/121)] [Code] DBM

2016
* Online RGB-D Tracking via Detection-Learning-Segmentation. Ning An, Xiao-Guang Zhao, Zeng-Guang Hou. ICPR 2016. [[paper](https://ieeexplore.ieee.org/abstract/document/7899805)] [Code] DLS
* Occlusion Aware Particle Filter Tracker to Handle Complex and Persistent Occlusions. Kourosh Meshgia, Shin-ichi Maedaa, Shigeyuki Obaa, Henrik Skibbea, Yu-zhe Lia, Shin Ishii. CVIU 2016. [[paper](https://ishiilab.jp/member/meshgi-k/files/ai/prl14/OAPF.pdf)] [Code] OAPF
* 3D Part-Based Sparse Tracker with Automatic Synchronization and Registration. Adel Bibi, Tianzhu Zhang, Bernard Ghanem. CVPR 2016. [[paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Bibi_3D_Part-Based_Sparse_CVPR_2016_paper.html)] [[Code](https://github.com/adelbibi/3D-Part-Based-Sparse-Tracker-with-Automatic-Synchronization-and-Registration)] 3D-T

2015
* Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling. Massimo Camplani, Sion Hannuna, Majid Mirmehdi, Dima Damen, Adeline Paiement, Lili Tao, Tilo Burghardt. BMVC 2015. [[paper](https://d1wqtxts1xzle7.cloudfront.net/85352060/573f8735e984c25db8b2a805235bf22dc042-libre.pdf?1651508251=&response-content-disposition=inline%3B+filename%3DReal_time_RGB_D_Tracking_with_Depth_Scal.pdf&Expires=1718988659&Signature=JaYE6NgAC6zmBSu2tCQjJhVqXXSkIAY0u6yRSJvIJ7s8g-h9Xkqixnlk3QRdNjyoZPSFmMv-hAB6LNva-T0RzA8y2vNGAlBTR5l1HT6Tv5blcIlm5daDCXdNDTpM5SAdZDd3-4-x1HthLefubKfNVRF4W4Um29OtFWcEGzfKbasPCThbKV-Jf5F~43tZvBPtuc1EN2fajbqK3RlgtbAaa8QNN5RwgWoAdzBWareCsR1fVhvu5K-7Bcgb8gK9MzuZY2rdmzUvNu8Z~hpq5dWAnrmK21I7wR5~XhBgXLPh4kM2IaZAOJqTTAoC51PlQ-HOHUOVriFqUI1qvoJNHOo~Rg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)] [Code] DS-KCF
* Robust Object Tracking Using Color and Depth Images with a Depth Based Occlusion Handling and Recovery. Ping Ding, Yan Song. FSKD 2015. [[paper](https://ieeexplore.ieee.org/abstract/document/7382068)] [Code] DOHR
* Using Consistency of Depth Gradient to Improve Visual Tracking in RGB-D sequences. Huizhang Shi, Changxin Gao, Nong Sang. CAC 2015. [[paper](https://ieeexplore.ieee.org/abstract/document/7382555)] [Code] CDG
* 

2014
* Multi-Cue Based Tracking. Qi Wang, Jianwu Fang, Yuan Yuan. Multi-Cue Based Tracking. Neurocomputing  2014. [[paper](https://www.sciencedirect.com/science/article/pii/S0925231213010801)] [Code] MCBT
* Automatic Video Segmentation and Object Tracking with Real-Time RGB-D Data. I-Kuei Chen, Szu-Lu Hsu, Chung-Yu Chi, and Liang-Gee Chen. ICCE 2014. [[paper](https://ieeexplore.ieee.org/abstract/document/6776097)] [Code] AVSOT
* Occlusion Handling Method for Object Tracking Using RGB-D data. Ariel Benou, Itay Benou, Rami Hagage. IEEEI 2014. [[paper](https://ieeexplore.ieee.org/abstract/document/7005857)] [Code] OHM

2013
* Tracking Revisited using RGBD Camera: Unified Benchmark and Baselines. Shuran Song Jianxiong Xiao. ICCV 2013.[[paper](https://openaccess.thecvf.com/content_iccv_2013/html/Song_Tracking_Revisited_Using_2013_ICCV_paper.html)] [Code] PTB.

2012
* Adaptive Multi-cue 3D Tracking of Arbitrary Objects. Germ√°n Mart√≠n Garc√≠a, Dominik Alexander Klein, J√∂rg St√ºckler, Simone Frintrop, Armin B. Cremers. JDOS 2012. [[paper](https://link.springer.com/chapter/10.1007/978-3-642-32717-9_36)] [Code] AMCT
* 



### RGB-E Tracking

2024
* CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras. Yabin Zhu, Xiao Wang, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang. Arxiv 2024. [[Paper](https://arxiv.org/abs/2401.02826)] [[Code](https://github.com/Event-AHU/Cross_Resolution_SOT)] CRSOT
* Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking. Pengcheng Shao, Tianyang Xu, Xuefeng Zhu, Xiaojun Wu, and Josef Kittler. PRCV 2024. [[Paper](https://arxiv.org/abs/2409.17560)] [[Code]()] DS-MESA
* eMoE-Tracker: Environmental MoE-based Transformer for Robust Event-guided Object Tracking. Yucheng Chen and Lin Wang. Arxiv 2024. [[Paper](https://arxiv.org/abs/2406.20024)] [[Code](https://vlislab22.github.io/eMoE-Tracker/)] eMoE-Tracker
* ED-DCFNet: An Unsupervised Encoder-decoder Neural Model for Event-driven Feature Extraction and Object Tracking. Raz Ramon, Hadar Cohen-Duwek, Elishai Ezra Tsur. CVPRW 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024W/EVW/papers/Ramon_ED-DCFNet_An_Unsupervised_Encoder-decoder_Neural_Model_for_Event-driven_Feature_Extraction_CVPRW_2024_paper.pdf)] [[Code](https://github.com/NBELab/UnsupervisedTracking)] ED-DCFNet
* Mamba-FETrack: Frame-Event Tracking via State Space Model. Ju Huang, Shiao Wang, Shuai Wang, Zhe Wu, Xiao Wang, Bo Jiang. PRCV 2024. [[Paper](https://arxiv.org/abs/2404.18174)] [[Code](https://github.com/Event-AHU/Mamba_FETrack)] Mamba-FETrack
* Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion. Hongze Sun, Rui Liu, Wuque Cai, Jun Wang, Yue Wang, Huajin Tang, Yan Cui, Dezhong Yao, Daqing Guo. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.17903)] [Code] MMHT.
* SiamEFT: adaptive-time feature extraction hybrid network for RGBE multi-domain object tracking. Shuqi Liu, Gang Wang, Yong Song, Jinxiang Huang, Yiqian Huang, Ya Zhou and Shiqiang Wan. Frontiers in Neuroscience 2024. [[Paper](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1453419/full)] [Code] SiamEFT.
* TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking. Pengcheng Shao, Tianyang Xu, Zhangyong Tang, Linze Li, Xiao-Jun Wu, Josef Kittler. Arxiv 2024. [[Paper](https://arxiv.org/abs/2405.05004)] [[Code](https://github.com/SSSpc333/TENet)] TENet


2023
* Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers. Zhiyu Zhu, Junhui Hou, Dapeng Oliver Wu. ICCV 2023. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Cross-Modal_Orthogonal_High-Rank_Augmentation_for_RGB-Event_Transformer-Trackers_ICCV_2023_paper.html)] [[Code](https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker)] HRCEUTrack
* Frame-Event Alignment and Fusion Network for High Frame Rate Tracking. Jiqing Zhang, Yuanchen Wang, Wenxi Liu, Meng Li, Jinpeng Bai, Baocai Yin, Xin Yang. CVPR 2023. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Frame-Event_Alignment_and_Fusion_Network_for_High_Frame_Rate_Tracking_CVPR_2023_paper.html)] [[Code](https://github.com/Jee-King/AFNet)] AFNet
* VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. Xiao Wang, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, and Feng Wu. TCYB 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10284004)] [[Code](https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark)] VisEvent

2022
* Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. Chuanming Tang, Xiao Wang, Ju Huang, Bo Jiang, Lin Zhu, Jianlin Zhang, Yaowei Wang, Yonghong Tian. Arxiv 2022. [[Paper](https://arxiv.org/abs/2211.11010)] [[Code](https://github.com/Event-AHU/COESOT)] COESOT

2021
* Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. Jiqing Zhang, Kai Zhao, Bo Dong, Yingkai Fu, Yuxin Wang, Xin Yang, Baocai Yin. The Visual Computer 2021. [[Paper](https://link.springer.com/article/10.1007/s00371-021-02237-9)] [Code] CFE
* Object Tracking by Jointly Exploiting Frame and Event Domain. Jiqing Zhang, Xin Yang, Yingkai Fu, Xiaopeng Wei, Baocai Yin, Bo Dong. ICCV 2021. [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Object_Tracking_by_Jointly_Exploiting_Frame_and_Event_Domain_ICCV_2021_paper.html)] [[Code](https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking)] FE108
* 

### RGB-L Tracking
2024
* Context-Aware Integration of Language and Visual References for Natural Language Tracking. Yanyan Shao, Shuting He, Qi Ye, Yuchao Feng, Wenhan Luo, Jiming Chen. CVPR 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Shao_Context-Aware_Integration_of_Language_and_Visual_References_for_Natural_Language_CVPR_2024_paper.html)] [[Code](https://github.com/twotwo2/QueryNLT)] QueryNLT.
* Consistencies are All You Need for Semi-supervised Vision-Language Tracking.  Jiawei Ge, Jiuxin Cao, Xuelin Zhu, Xinyu Zhang, Chang Liu, Kun Wang, Bo Liu. ACM MM 2024. [[Paper](https://openreview.net/forum?id=jLJ3htNxVX)] [Code] ATTracker
* DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM. Xuchen Li, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang. CVPRW 2024. [[Paper](https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Li_DTLLM-VLT_Diverse_Text_Generation_for_Visual_Language_Tracking_Based_on_CVPRW_2024_paper.html)] [[Code](https://github.com/Xuchen-Li/DTLLM-VLT)] DTLLM-VLT.
* * Divert More Attention to Vision-Language Object Tracking. Mingzhe Guo, Zhipeng Zhang, Liping Jing, Haibin Ling, Heng Fan. TPAMI 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10547435)] [[Code](https://github.com/JudasDie/SOTS?tab=readme-ov-file)] VLT_OST
* Multimodal Features Alignment for Vision‚ÄìLanguage Object Tracking. Ping Ye, Gang Xiao, Jun Liu. Remote Sensing 2024. [[Paper](https://www.mdpi.com/2072-4292/16/7/1168)] [Code] MFAVLT
* One-Stream Stepwise Decreasing for Vision-Language Tracking. Guangtong Zhang, Bineng Zhong, Qihua Liang, Zhiyi Mo, Ning Li, Shuxiang Song. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10510485)] [[Code]()] OSDT
* Textual Tokens Classification for Multi-Modal Alignment in Vision-Language Tracking. Zhongjie Mao; Yucheng Wang; Xi Chen; Jia Yan. ICASSP 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10446122)] [[Code](https://github.com/jankin987/ttctrack)] TTCTrack.
* Toward Unified Token Learning for Vision-Language Tracking. Zheng, Yaozong and Zhong, Bineng and Liang, Qihua and Li, Guorong and Ji, Rongrong and Li, Xianxian. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10208210)] [Code] MMTrack.
* Unifying Visual and Vision-Language Tracking via Contrastive Learning. Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Jinpeng Zhang, Mengxue Kang. AAAI 2024. [[Paper](https://arxiv.org/abs/2401.11228)] [[Code](https://github.com/OpenSpaceAI/UVLTrack)] UVLTrack
* VastTrack: Vast Category Visual Object Tracking. Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, and Libo Zhang. Arxiv 2024. [[Paper](https://arxiv.org/abs/2403.03493)] [[Code](https://github.com/HengLan/VastTrack)] VastTrack.
* WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark. Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, Yanfeng Wang. Arxiv 2024. [[Paper](https://arxiv.org/pdf/2405.19818)] [[Code](https://github.com/983632847/Awesome-Multimodal-Object-Tracking?tab=readme-ov-file)]

2023
* A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship. Shiyu Hu, Dailing Zhang, Meiqi Wu, Xiaokun Feng, Xuchen Li, Xin Zhao, Kaiqi Huang. NIPS 2023. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html)] [[Code](http://videocube.aitestunion.com/)] MGIT.
* All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang. ICCV 2023. [[Paper](https://dl.acm.org/doi/abs/10.1145/3581783.3611803)] [[Code](https://github.com/983632847/All-in-One)] ALl in One
* Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking. Jiawei Ge, Xiangmei Chen, Jiuxin Cao, Xuelin Zhu, Bo Liu. Arxiv 2023. [[Paper](https://arxiv.org/abs/2311.17085)] [[Code]()] SATracker
* CiteTracker: Correlating Image and Text for Visual Tracking. Xin Li, Yuqing Huang, Zhenyu He, Yaowei Wang, Huchuan Lu, Ming-Hsuan Yang. ICCV 2023. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_CiteTracker_Correlating_Image_and_Text_for_Visual_Tracking_ICCV_2023_paper.html)] [[Code](https://github.com/NorahGreen/CiteTracker)] CiteTracker.
* Joint Visual Grounding and Tracking with Natural Language Specifcation. Li Zhou, Zikun Zhou, Kaige Mao, Zhenyu He. CVPR 2023. [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.html)] [[Code](https://github.com/lizhou-cs/JointNLT)] JointNLT.
* Multi-Modal Object Tracking with Vision-Language Adaptive Fusion and Alignment. Zuo, Jixiang and Wu, Tao and Shi, Meiping and Liu, Xueyan and Zhao, Xijun. RICAI 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10489325)] [Code] VLATrack.
* One-Stream Vision-Language Memory Network for Object Tracking. Zhang, Huanlong and Wang, Jingchao and Zhang, Jianwei and Zhang, Tianzhu and Zhong, Bineng. TMM 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10149530)] [Code] OVLM.
* Tracking by Natural Language Specification with Long Short-term Context Decoupling. Ma, Ding and Wu, Xiangqian. ICCV 2023. [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Tracking_by_Natural_Language_Specification_with_Long_Short-term_Context_Decoupling_ICCV_2023_paper.html)] [[Code]()] DecoupleTNL
* Transformer vision-language tracking via proxy token guided cross-modal fusion. Haojie Zhao, Xiao Wang, Dong Wang, Huchuan Lu, Xiang Ruan. PRL 2023. [[Paper](https://www.sciencedirect.com/science/article/pii/S0167865523000545)] [Code] PTG
* Unified Transformer With Isomorphic Branches for Natural Language Tracking. Rong Wang, Zongheng Tang, Student Member, IEEE, Qianli Zhou, Xiaoqian Liu. TCSVT 2023. [[Paper](https://ieeexplore.ieee.org/abstract/document/10159158)] [Code] TransNLT

2022
* Cross-modal Target Retrieval for Tracking by Natural Language. Li, Yihao and Yu, Jun and Cai, Zhongpeng and Pan, Yuwen. CVPRW 2022. [[Paper](https://openaccess.thecvf.com/content/CVPR2022W/ODRUM/html/Li_Cross-Modal_Target_Retrieval_for_Tracking_by_Natural_Language_CVPRW_2022_paper.html)] [Code] AdaRS
* Divert More Attention to Vision-Language Tracking. Mingzhe Guo, Zhipeng Zhang, Heng Fan, Liping Jing. NIPS 2022. [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/1c8c87c36dc1e49e63555f95fa56b153-Abstract-Conference.html)] [[Code](https://github.com/JudasDie/SOTS?tab=readme-ov-file)] VLT_TT

2021
* Capsule-based Object Tracking with Natural Language Specification. Ding Ma, Xiangqian Wu. ACM MM 2021. [[Paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475349)] [Code] CapsuleNLT
* LaSOT: A High-quality Large-scale Single Object Tracking Benchmark. Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan, Haibin Ling. IJCV 2021. 
[[Paper](https://link.springer.com/article/10.1007/s11263-020-01387-y)] [[Code](https://github.com/HengLan/LaSOT_Evaluation_Toolkit)] LaSOT_EXT.
* Siamese Natural Language Tracker: Tracking by Natural Language Descriptions with Siamese Trackers. Qi Feng, Vitaly Ablavsky, Qinxun Bai, Stan Sclaroff. CVPR 2021. [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Feng_Siamese_Natural_Language_Tracker_Tracking_by_Natural_Language_Descriptions_With_CVPR_2021_paper.html)] [[Code](https://github.com/fredfung007/snlt)] SNLT.
* Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark. Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu. CVPR 2021. [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.html)] [Code] TNL2K.

2020
* Real-time visual object tracking with natural language description. Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, and Stan Sclarof. WACV 2020. [[Paper](https://openaccess.thecvf.com/content_WACV_2020/html/Feng_Real-time_Visual_Object_Tracking_with_Natural_Language_Description_WACV_2020_paper.html)] [Code] RTTNLD

2019
* LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking. Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling. CVPR 2019. [[Paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Fan_LaSOT_A_High-Quality_Benchmark_for_Large-Scale_Single_Object_Tracking_CVPR_2019_paper.html)] [[Code](https://github.com/HengLan/LaSOT_Evaluation_Toolkit)] LaSOT.
* Robust visual object tracking with natural language region proposal network. Feng, Qi and Ablavsky, Vitaly and Bai, Qinxun and Sclaroff, Stan. Arxiv 2019. [[Paper](https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=Robust+visual+object+tracking+with+natural+language+region+proposal+network&btnG=)] [Code] RVTNLN. can not download

  
2017
* Tracking by Natural Language Specification. Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek, Arnold W.M. Smeulders. CVPR 2017. OTB99-L. [[Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Tracking_by_Natural_CVPR_2017_paper.pdf)] [[Code](https://github.com/QUVA-Lab/lang-tracker)]

### RGB-NIR Tracking

* Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset. Lei Liu, Mengya Zhang, Cheng Li, Chenglong Li, and Jin Tang. TNNLS 2024. CMOTB. [[Paper](https://github.com/mmic-lcl/Datasets-and-benchmark-code)] [[Code](https://github.com/xfarawayx/CMOTB_Toolkit)]
* Prototype-based Cross-Modal Object Tracking. Lei Liu, Chenglong Li, Futian Wang, Longfeng Shen, and Jin Tang. Arxiv 2024. ProtoTrack. [[Paper](https://arxiv.org/pdf/2312.14471)] [[Code](https://github.com/liulei970507/ProtoTrack)]
* Cross-Modal Object Tracking: Modality-Aware Representations and A Unified Benchmark. Chenglong Li, Tianhao Zhu, Lei Liu, Xiaonan Si, Zilin Fan, Sulan Zhai. AAAI 2022. CMOTB. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/20016)] [[Code](https://github.com/xfarawayx/CMOTB_Toolkit)]

### RGB-S Tracking

* RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer Tracker. Yunfeng Li, Bo Wang*, Jiuran Sun, Xueyi Wu, Ye Li. Arxiv 2024. RGBS50. [[Paper](https://arxiv.org/pdf/2406.07189)] [[Code](https://github.com/LiYunfengLYF/RGBS50)]

### RGB-Multi Tracking
* Visual and Language Collaborative Learning for RGBT Object Tracking. Jiahao Wang, Fang Liu, Licheng Jiao, Yingjia Gao, Hao Wang, Shuo Li,, Lingling Li,, Puhua Chen,and Xu Liu. TCSVT 2024. [[Paper](https://ieeexplore.ieee.org/abstract/document/10620225)] [Code]
* UniMod1K: Towards a More Universal Large-Scale Dataset and Benchmark for Multi-modal Learning. Xue-Feng Zhu, Tianyang Xu, Zongtao Liu, Zhangyong Tang, Xiao-Jun Wu & Josef Kittler. IJCV 2024. [[Paper](https://link.springer.com/article/10.1007/s11263-024-01999-8)] [[Code](https://github.com/xuefeng-zhu5/UniMod1K)] 
* WebUAV-3M: A Benchmark for Unveiling the Power of Million-Scale Deep UAV Tracking. Chunhui Zhang, Guanjie Huang, Li Liu, Shan Huang, Yinan Yang, Xiang Wan, Shiming Ge, and Dacheng Tao. TPAMI 2023. [[Paper](https://ieeexplore.ieee.org/document/10004511)]



## ü•á Competition
 1. [AntiUAV 1st](https://anti-uav.github.io/)
	The first AntiUAV challenge is a multi-modal challenge. It was held in 2020.  The website for the 1st AntiUAV is covered by the newest one. The winner is team *xiaobaibai*. One of its member is [Tianyang Xu](https://xu-tianyang.github.io/)
 2. [VOT-RGBT2019](https://votchallenge.net/vot2019/)
 	The first competition in the RGBT tracking community. It was held in 2019. The winner is *SiamDW*.
 3. [VOT-RGBT2020]
    A competition in the RGBT tracking community. It was held in 2020. The winner is *DFAT*. One of its member is [Zhangyong Tang](https://github.com/Zhangyong-Tang/)


## ‚öì Awesome Repositories
* [Awesome-Multimodal-Object-Tracking](https://github.com/983632847/Awesome-Multimodal-Object-Tracking)
* [RGB-Event-Benchmarks](https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark)
* [RGBT-Thermal-Results-Datasets-Methods](https://github.com/Zhangyong-Tang/RGBT-Tracking-Results-Datasets-and-Methods)

## üßë‚Äç‚öïÔ∏è Acknowledgements
This work is supported by [PRCI-Lab](https://github.com/PRCI-Lab), which is an outstanding and also fast-developing group. Please feel free to find out more information through its home page.


## ü´∞ Questions

If you have any questions, please feel free to start the issue, or contact me at zhangyong_tang_jnu@163.com (wechat: Tzy18861871359  is also welcomed).


